# EmberCortex - llama.cpp Server Configuration
# This file is sourced by the systemd service
# Edit this file to change model, context length, etc.
# After editing, restart the service: sudo systemctl restart llama-server

# Path to llama.cpp build directory
LLAMA_CPP_DIR=/home/anelson/ggml-org/llama.cpp/build-cuda

# Model path (GGUF format)
#MODEL_PATH=/home/anelson/llm_models/Llama-3.3-70B-Instruct-Q4_K_M.gguf
MODEL_PATH=/home/anelson//llm_models/gpt-oss-120b-mxfp4/gpt-oss-120b-mxfp4-00001-of-00003.gguf

# Server settings
HOST=0.0.0.0
PORT=8080

# Context length (adjust based on your needs and VRAM)
CONTEXT_LENGTH=64000

# GPU layers to offload (99 = all layers)
GPU_LAYERS=99

# CPU threads for non-GPU work
THREADS=16

# Batch size (reduce if OOM)
BATCH_SIZE=2048

# Disable mmap (recommended for DGX Spark)
NO_MMAP=true

# Enable flash attention (if supported by model)
FLASH_ATTN=false

# Additional flags (optional)
# EXTRA_FLAGS=--verbose
EXTRA_FLAGS=
